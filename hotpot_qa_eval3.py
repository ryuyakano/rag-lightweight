# ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†å‰²ã‚’ä¸Šæ‰‹ãè¡Œã†ã“ã¨ã«æˆåŠŸã—ãŸ
# OpenAIã®APIãŒä½¿ãˆã¦ã„ãªã„ã£ã¦ã„ã†ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸãŒã€ã‚³ãƒ¼ãƒ‰å¤‰æ›´ã›ãšä½•å›ã‹å®Ÿè¡Œã—ã¦ãŸã‚‰é€šã£ãŸ
# ç„¡äº‹å®Ÿè¡Œå®Œäº†
# pip install "ragas[openai]==0.1.0â€ã‚¨ãƒ©ãƒ¼å‡ºãŸã‚‰ã“ã‚Œå…¥ã‚Œã¦ï¼ï¼
import time, requests, wandb
from datasets import load_dataset, Dataset
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors
from ragas import evaluate
from ragas.metrics import (
    answer_relevancy, context_precision, context_recall, faithfulness
)
# openAIã®ãƒ¢ãƒ‡ãƒ«ã‚‚è©•ä¾¡ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã¿ã‚‹
from openai import OpenAI
import os
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))



def rag_generate_openai(question: str, contexts: list[str], model_name: str) -> str:
    ctx_block = "\n\n".join(f"Document {i+1}:\n{c}" for i, c in enumerate(contexts))
    prompt = (
        f"Please answer the following question based on the documents below.\n\n"
        f"{ctx_block}\n\nQuestion: {question}\n\nAnswer:"
    )

    response = client.chat.completions.create(model=model_name,  # "gpt-3.5-turbo" ã§ã‚‚å¯
    messages=[{"role": "user", "content": prompt}],
    temperature=0)
    return response.choices[0].message.content.strip()
# ---------------- è¨­å®š ----------------
OLLAMA_API_URL = "http://ollama:11434/api/generate"
# MODEL_NAME     = "gemma3:12b"
# MODEL_NAME     = "llama3.3:70b"
MODEL_NAME     = "llama3:8b"
# MODEL_NAME     = "gemma3:27b"
# MODEL_NAME = "gpt-4o"  # OpenAI ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ãŸã„å ´åˆ



N_SAMPLES      = 10
TOP_K          = 5
PASSAGE_LEN    = 1
# --------------------------------------
# â‘  ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
ds_eval   = load_dataset("hotpot_qa", "fullwiki", split=f"validation[:{N_SAMPLES}]")
ds_corpus = load_dataset("hotpot_qa", "fullwiki", split="train[:3000]+validation")
# ds_corpus = load_dataset("hotpot_qa", "fullwiki", split="train[:30]")
# EVAL_NAME ã‚’ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰å‹•çš„ã«ç”Ÿæˆ
EVAL_NAME = f"{MODEL_NAME.replace(':', '_')}_top{TOP_K}_n{N_SAMPLES}_len{PASSAGE_LEN}_corpus{len(ds_corpus)}"

wandb.init(
    # mode="disabled",  # â† ãƒ­ã‚°ã‚’å–ã‚ŠãŸããªã„ã¨ãã¯ "disabled"
    project="rag-eval-hotpotQA", 
    name=EVAL_NAME, config={
    "model": MODEL_NAME,
    "samples": N_SAMPLES,
    "top_k": TOP_K,
})

sample_table = wandb.Table(columns=["question", "contexts", "answer", "reference"])


# â‘¡ ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆï¼ˆæ–‡ã‚’ 4 æœ¬ãšã¤ã¾ã¨ã‚ã¦ 1 ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
def chunked(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

flat_docs = []
for ex in ds_corpus:
    titles = ex["context"]["title"]
    passages = ex["context"]["sentences"]

    for title, sents in zip(titles, passages):
        for chunk in chunked(sents, PASSAGE_LEN):
            flat_docs.append(f"{title}: " + " ".join(chunk))

# print("ç·ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸æ•°:", len(flat_docs))
print("ä½œæˆã•ã‚ŒãŸãƒ‘ãƒƒã‚»ãƒ¼ã‚¸æ•°:", len(flat_docs))
# print("ã‚µãƒ³ãƒ—ãƒ«:")
# for i in range(10):
#     print(f"[{i}] {flat_docs[i]}")

# â‘¢ åŸ‹ã‚è¾¼ã¿ + æœ€è¿‘å‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
embedder = SentenceTransformer("all-mpnet-base-v2",device="cuda")
doc_vecs = embedder.encode(flat_docs,batch_size=128, show_progress_bar=True, num_workers=4)
index    = NearestNeighbors(n_neighbors=TOP_K, metric="cosine").fit(doc_vecs)

def retrieve(question: str):
    q_vec = embedder.encode([question])
    _, idxs = index.kneighbors(q_vec)
    return [flat_docs[i] for i in idxs[0]]

def rag_generate(question: str, contexts: list[str]) -> str:
    ctx_block = "\n\n".join(f"ã€æ–‡æ›¸{i+1}ã€‘\n{c}" for i, c in enumerate(contexts))
    # prompt = f"ä»¥ä¸‹ã®æ–‡æ›¸ã‚’å‚è€ƒã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n\n{ctx_block}\n\nè³ªå•: {question}\n\nç­”ãˆ:"
    prompt = f"Please answer the following question based on the documents below.\n\n{ctx_block}\n\nQuestion: {question}\n\nAnswer:"
    r = requests.post(OLLAMA_API_URL, json={"model": MODEL_NAME, "prompt": prompt, "stream": False})
    r.raise_for_status()
    return r.json()["response"].strip()
    # return r.json().response.strip()

# â‘£ å®Ÿè¡Œãƒ«ãƒ¼ãƒ—
records = []
t0 = time.time()
for i, ex in enumerate(ds_eval):
    q   = ex["question"]
    gt  = ex["answer"]
    ctx = retrieve(q)
    # print(ctx)
    # print(gt)
    if MODEL_NAME.lower() in ["gpt-3.5-turbo", "gpt-4o"]: #ã“ã“ã¯ä½¿ã†ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        # print("openAI!!")
        ans = rag_generate_openai(q, ctx, MODEL_NAME)
    else:
        ans = rag_generate(q, ctx)  # Ollamaç”¨
    # ans = rag_generate(q, ctx)
    sample_table.add_data(q, ctx, ans, gt)
    records.append({
        "question":      q,
        "contexts":      ctx,
        "answer":        ans,
        "ground_truth":  gt[0],
        "reference":     gt
    })
    print(f"[{i+1}/{N_SAMPLES}] done")

print("å‡¦ç†æ™‚é–“:", round(time.time() - t0, 1), "ç§’")

# â‘¤ RAGAS è©•ä¾¡
rag_ds  = Dataset.from_list(records)
# metrics = [answer_relevancy, faithfulness, context_precision, context_recall]
metrics = [answer_relevancy, faithfulness, context_precision]
results = evaluate(rag_ds, metrics=metrics)

print(results)
# è©•ä¾¡çµæœã‚’è¾æ›¸ã¨ã—ã¦å–å¾—
print("\n===== RAGAS scores =====")

scores_dict = {}
for name in results:               # name ã¯ 'answer_relevancy' ãªã©ã®æ–‡å­—åˆ—
    val = results[name]            # ã‚¹ã‚³ã‚¢ã‚’å–å¾—
    print(f"{name:20s}: {val:.3f}")
    scores_dict[name] = val        # wandb ç”¨ã«ä¿å­˜


wandb.log({**scores_dict, "samples": sample_table})
wandb.finish()




# # hotpotQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå‹•ã„ãŸ(wandbã¨RAGASå¯¾å¿œã—ã¦ã„ãªã„)
# # wandbã‚’å¯¾å¿œã•ã›ã‚‹
# import time, requests, wandb
# from datasets import load_dataset, Dataset
# from sentence_transformers import SentenceTransformer
# from sklearn.neighbors import NearestNeighbors
# from ragas import evaluate
# from ragas.metrics import (
#     answer_relevancy, context_precision, context_recall, faithfulness
# )
# # ---------------- è¨­å®š ----------------
# OLLAMA_API_URL = "http://ollama:11434/api/generate"
# # MODEL_NAME = "gemma3:12b"
# MODEL_NAME = "llama3:8b"
# EVAL_NAME      = "ragas-gemma3-amnesty"
# N_SAMPLES  = 2          # è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°
# TOP_K      = 5          # æ¤œç´¢ã§è¿”ã™ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸æ•°
# PASSAGE_LEN = 4         # æ–‡ã‚’ã¾ã¨ã‚ã‚‹æ•°
# # --------------------------------------

# wandb.init(project="rag-eval", name=EVAL_NAME, config={ "model": MODEL_NAME,
#         # "dataset": DS_NAME,
#         # "config_name": CONFIG_NAME,
#         "samples": N_SAMPLES,
#         "top_k": TOP_K,})

# sample_table = wandb.Table(columns=["question", "contexts", "answer", "reference"])

# # â‘  ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ds_eval   = load_dataset("hotpot_qa", "fullwiki", split=f"validation[:{N_SAMPLES}]")
# ds_corpus = load_dataset("hotpot_qa", "fullwiki", split="train[:3000]+validation")

# # â‘¡ ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆï¼ˆæ–‡ã‚’ 4 æœ¬ãšã¤ã¾ã¨ã‚ã¦ 1 ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
# def chunked(lst, n):
#     for i in range(0, len(lst), n):
#         yield lst[i:i+n]

# flat_docs = []
# for ex in ds_corpus:
#     for title, sents, *_ in ex["context"]:
#         for chunk in chunked(sents, PASSAGE_LEN):
#             flat_docs.append(f"{title}: " + " ".join(chunk))

# print("ç·ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸æ•°:", len(flat_docs))   # ä¾‹: 150 K ä»¥ä¸Š

# # â‘¢ åŸ‹ã‚è¾¼ã¿ + æœ€è¿‘å‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
# embedder = SentenceTransformer("all-mpnet-base-v2")   # â†’ 768 æ¬¡å…ƒ
# doc_vecs = embedder.encode(flat_docs, show_progress_bar=True)
# index    = NearestNeighbors(n_neighbors=TOP_K, metric="cosine").fit(doc_vecs)

# def retrieve(question: str):
#     q_vec = embedder.encode([question])
#     _, idxs = index.kneighbors(q_vec)
#     return [flat_docs[i] for i in idxs[0]]

# def rag_generate(question: str, contexts: list[str]) -> str:
#     ctx_block = "\n\n".join(f"ã€æ–‡æ›¸{i+1}ã€‘\n{c}" for i, c in enumerate(contexts))
#     prompt = f"ä»¥ä¸‹ã®æ–‡æ›¸ã‚’å‚è€ƒã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n\n{ctx_block}\n\nè³ªå•: {question}\n\nç­”ãˆ:"
#     r = requests.post(OLLAMA_API_URL, json={"model": MODEL_NAME, "prompt": prompt, "stream": False})
#     r.raise_for_status()
#     return r.json()["response"].strip()

# # â‘£ å®Ÿè¡Œãƒ«ãƒ¼ãƒ—ï¼ˆä¾‹ï¼š5 ä»¶ã ã‘å›ã—ã¦ã¿ã‚‹ï¼‰
# records = []
# t0 = time.time()
# for i, ex in enumerate(ds_eval):
#     q   = ex["question"]
#     gt  = ex["answer"]
#     ctx = retrieve(q)
#     ans = rag_generate(q, ctx)
#     sample_table.add_data(q, ctx, ans, gt[0])
#     records.append({
#         "question":      q,
#         "contexts":      ctx,
#         "answer":        ans,
#         "ground_truths": gt,
#         "reference":     gt[0]  # ğŸ”¥ RAGAS workaround
#     })
#     print(f"[{i+1}/{N_SAMPLES}] done")

# print("å‡¦ç†æ™‚é–“:", round(time.time()-t0, 1), "ç§’")

# rag_ds  = Dataset.from_list(records)
# metrics = [answer_relevancy, faithfulness, context_precision, context_recall]
# # ãƒ¬ã‚³ãƒ¼ãƒ‰ã®çµæœã¨ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã‹ã‚‰çµæœã‚’è¨ˆç®—
# results = evaluate(rag_ds, metrics=metrics)

# elapsed = time.time() - t0


# print("\n===== RAGAS scores =====")
# scores_dict = {}

# # for score in results.scores:  # â† List[dict]
# #     name = score["name"]       # ã‚¹ã‚³ã‚¢åï¼ˆä¾‹: "faithfulness"ï¼‰
# #     val  = float(score["score"])  # ã‚¹ã‚³ã‚¢å€¤
# #     print(f"{name:20s}: {val:.3f}")
# #     scores_dict[name] = val

# scores_dict = {}
# for name in results:               # name ã¯ 'answer_relevancy' ãªã©ã®æ–‡å­—åˆ—
#     val = results[name]            # ã‚¹ã‚³ã‚¢ã‚’å–å¾—
#     print(f"{name:20s}: {val:.3f}")
#     scores_dict[name] = val        # wandb ç”¨ã«ä¿å­˜

# wandb.log({**scores_dict, "samples": sample_table})
# wandb.finish()
# # `records` ã‚’ãã®ã¾ã¾ RAGAS ã«æ¸¡ã—ã¦è©•ä¾¡ã—ã¦ã‚‚è‰¯ã„
