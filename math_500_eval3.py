# openAIã®apiã‚’ä½¿ç”¨ã—ãŸå®Ÿé¨“ã‚’è¡Œã†

import openai
import requests
import json
import time
import re
from datasets import load_dataset

import wandb  # ğŸ”¸ è¿½åŠ 


OLLAMA_API_URL = "http://ollama:11434/api/generate"
# MODEL_NAME = "llama3:8b"
# MODEL_NAME = "llama3.3:70b"
MODEL_NAME = "gemma3:27b"
# MODEL_NAME = "gemma3:12b"

HEADERS = {"Content-Type": "application/json"}

# æœ€åˆã®10å•ã§ãƒ†ã‚¹ãƒˆ
dataset = load_dataset("HuggingFaceH4/MATH-500", split="test[:50]")

# LLMã«åˆ¤æ–­ã•ã›ã‚‹ç”¨ã®é–¢æ•°ã‚’è¿½åŠ 
def ask_judgment(question, expected, reply):
    judge_prompt = (
        f"[å•é¡Œ]\n{question}\n\n"
        f"[æœŸå¾…ã•ã‚Œã‚‹æ­£è§£]\n{expected}\n\n"
        f"[ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›]\n{reply}\n\n"
        "ã“ã®ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã‚‹æ­£è§£ã¨ä¸€è‡´ã™ã‚‹ã‹ã©ã†ã‹ã‚’ã€Œæ­£è§£ã€ã¾ãŸã¯ã€Œä¸æ­£è§£ã€ã§ç­”ãˆã¦ãã ã•ã„ã€‚"
        "ãã®ç†ç”±ã‚‚ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    )

    payload = {
        "model": MODEL_NAME,
        "prompt": judge_prompt,
        "stream": False
    }
    response = requests.post(OLLAMA_API_URL, headers=HEADERS, data=json.dumps(payload))
    if response.status_code == 200:
        result = response.json()
        return result.get("response", "").strip()
    else:
        return f"åˆ¤å®šå¤±æ•—: {response.status_code}"

# wandb åˆæœŸåŒ–
wandb.init(project="math-llm-eval", name=f"eval-{MODEL_NAME}", config={
    "model": MODEL_NAME,
    "dataset": "MATH-500",
    "num_samples": 50
})


def ask_judgment_openai(question, expected, reply):
    judge_prompt = (
        f"[å•é¡Œ]\n{question}\n\n"
        f"[æœŸå¾…ã•ã‚Œã‚‹æ­£è§£]\n{expected}\n\n"
        f"[ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›]\n{reply}\n\n"
        "ã“ã®ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã‚‹æ­£è§£ã¨ä¸€è‡´ã™ã‚‹ã‹ã©ã†ã‹ã‚’ã€Œæ­£è§£ã€ã¾ãŸã¯ã€Œä¸æ­£è§£ã€ã§ç­”ãˆã¦ãã ã•ã„ã€‚"
        "ãã®ç†ç”±ã‚‚ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    )

    try:
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å³æ ¼ãªæ•°å¼æ¡ç‚¹è€…ã§ã™ã€‚æ­£ã—ã„å ´åˆã®ã¿æ­£è§£ã¨åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": judge_prompt}
            ],
            temperature=0,
            max_tokens=500
        )
        result = response.choices[0].message.content.strip()
        return result

    except Exception as e:
        return f"åˆ¤å®šå¤±æ•—ï¼ˆOpenAI APIã‚¨ãƒ©ãƒ¼ï¼‰: {e}"

# correct = 0
regex_correct = 0
llm_correct = 0
total_time = 0
max_time = 0
max_time_problem = ""

# --- è©•ä¾¡ãƒ«ãƒ¼ãƒ— ---
for i, example in enumerate(dataset, 1):
    question = example['problem'].strip()
    expected_raw = example['solution']
    expected_match = re.search(r"\\boxed\{(.+?)\}", expected_raw)
    expected_answer = expected_match.group(1).strip() if expected_match else None

    print(f"[{i}] å•é¡Œ: {question}")
    print(f"     æ­£è§£: {expected_answer}")

    prompt = (
        f"{question}\n"
        "Please solve the problem step-by-step. "
        "Then, write your final answer using LaTeX box format like \\boxed{{...}}."
    )

    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False
    }

    start_time = time.time()
    try:
        response = requests.post(OLLAMA_API_URL, headers=HEADERS, data=json.dumps(payload))
    except Exception as e:
        print(f"  ğŸš¨ é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        continue
    elapsed_time = time.time() - start_time
    total_time += elapsed_time

    if elapsed_time > max_time:
        max_time = elapsed_time
        max_time_problem = question

    if response.status_code == 200:
        result = response.json()
        reply = result.get("response", "").strip()
        print("  ãƒ¢ãƒ‡ãƒ«ã®å›ç­”:", reply)

        match = re.search(r"\\boxed\{(.+?)\}", reply)
        predicted_answer = match.group(1).strip() if match else None

        # --- è©•ä¾¡ ---
        is_regex_correct = False
        is_llm_correct = False

        if predicted_answer == expected_answer:
            print("  âœ… æ­£è¦è¡¨ç¾ä¸€è‡´ã§æ­£è§£")
            is_regex_correct = True
            regex_correct += 1
        else:
            if predicted_answer:
                print(f"  âŒ æ­£è¦è¡¨ç¾ä¸€è‡´ã›ãšï¼ˆæŠ½å‡º: {predicted_answer}ï¼‰")
            else:
                print("  âŒ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸ä¸€è‡´ï¼ˆ\\boxed{{...}} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰")

            # LLM ã«ã‚ˆã‚‹æŸ”è»Ÿè©•ä¾¡ã‚’å®Ÿæ–½
            # judgment = ask_judgment(question, expected_answer, reply)
            judgment = ask_judgment_openai(question, expected_answer, reply)
            print("  ğŸ¤– LLMã«ã‚ˆã‚‹åˆ¤å®š:", judgment)

            if "æ­£è§£" in judgment and "ä¸æ­£è§£" not in judgment:
                print("  âœ… LLMè©•ä¾¡ã§æ­£è§£ï¼ˆæŸ”è»Ÿåˆ¤å®šï¼‰")
                is_llm_correct = True
                llm_correct += 1
            else:
                print("  âŒ LLMè©•ä¾¡ã§ã¯ä¸æ­£è§£")

        print()
    else:
        print(f"  ğŸš¨ APIã‚¨ãƒ©ãƒ¼: {response.status_code}\n{response.text}")

    time.sleep(1)

# çµæœå‡ºåŠ›
total = len(dataset)
regex_accuracy = regex_correct / total * 100
combined_accuracy = (regex_correct + llm_correct) / total * 100
average_time = total_time / total

print("ğŸ“Š è©•ä¾¡çµæœ")
print(f"ãƒ¢ãƒ‡ãƒ«å: {MODEL_NAME}")
print(f"âœ… æ­£è¦è¡¨ç¾ä¸€è‡´ã®æ­£è§£æ•°: {regex_correct}/{total}ï¼ˆ{regex_accuracy:.2f}%ï¼‰")
print(f"âœ… LLMã«ã‚ˆã‚‹è¿½åŠ æ­£è§£æ•°: {llm_correct}ï¼ˆé™¤å¤–æ¸ˆã¿ï¼‰")
print(f"âœ… åˆè¨ˆæ­£è§£æ•°ï¼ˆé‡è¤‡ãªã—ï¼‰: {regex_correct + llm_correct}/{total}ï¼ˆ{combined_accuracy:.2f}%ï¼‰")
print(f"â±ï¸ å¹³å‡å¿œç­”æ™‚é–“: {average_time:.2f} ç§’")
print(f"ğŸ¢ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ãŸå•é¡Œ:\n{max_time_problem}")
print(f"   æ‰€è¦æ™‚é–“: {max_time:.2f} ç§’")

wandb.log({
    "total_samples": total,
    "regex_correct_total": regex_correct,
    "llm_correct_total": llm_correct,
    "regex_accuracy": regex_accuracy,
    "combined_accuracy": combined_accuracy,
    "average_response_time": average_time,
    "max_time_problem": max_time_problem,
    "max_time": max_time
})

wandb.finish()